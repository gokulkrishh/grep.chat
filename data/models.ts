const models = [
  {
    id: "x-ai/grok-code-fast-1",
    canonical_slug: "x-ai/grok-code-fast-1",
    hugging_face_id: "",
    name: "xAI: Grok Code Fast 1",
    created: 1756238927,
    description:
      "Grok Code Fast 1 is a speedy and economical reasoning model that excels at agentic coding. With reasoning traces visible in the response, developers can steer Grok Code for high-quality work flows.",
    context_length: 256000,
    architecture: {
      modality: "text-\u003Etext",
      input_modalities: ["text"],
      output_modalities: ["text"],
      tokenizer: "Grok",
      instruct_type: null,
    },
    pricing: {
      prompt: "0.0000002",
      completion: "0.0000015",
      request: "0",
      image: "0",
      web_search: "0",
      internal_reasoning: "0",
      input_cache_read: "0.00000002",
    },
    top_provider: {
      context_length: 256000,
      max_completion_tokens: 10000,
      is_moderated: false,
    },
    per_request_limits: null,
    supported_parameters: [
      "include_reasoning",
      "logprobs",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_logprobs",
      "top_p",
    ],
  },
  {
    id: "anthropic/claude-sonnet-4",
    canonical_slug: "anthropic/claude-4-sonnet-20250522",
    hugging_face_id: "",
    name: "Anthropic: Claude Sonnet 4",
    created: 1747930371,
    description:
      "Claude Sonnet 4 significantly enhances the capabilities of its predecessor, Sonnet 3.7, excelling in both coding and reasoning tasks with improved precision and controllability. Achieving state-of-the-art performance on SWE-bench (72.7%), Sonnet 4 balances capability and computational efficiency, making it suitable for a broad range of applications from routine coding tasks to complex software development projects. Key enhancements include improved autonomous codebase navigation, reduced error rates in agent-driven workflows, and increased reliability in following intricate instructions. Sonnet 4 is optimized for practical everyday use, providing advanced reasoning capabilities while maintaining efficiency and responsiveness in diverse internal and external scenarios.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)",
    context_length: 200000,
    architecture: {
      modality: "text+image-\u003Etext",
      input_modalities: ["image", "text", "file"],
      output_modalities: ["text"],
      tokenizer: "Claude",
      instruct_type: null,
    },
    pricing: {
      prompt: "0.000003",
      completion: "0.000015",
      request: "0",
      image: "0.0048",
      web_search: "0",
      internal_reasoning: "0",
      input_cache_read: "0.0000003",
      input_cache_write: "0.00000375",
    },
    top_provider: {
      context_length: 200000,
      max_completion_tokens: 64000,
      is_moderated: false,
    },
    per_request_limits: null,
    supported_parameters: [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "stop",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p",
    ],
  },
  {
    id: "google/gemini-2.5-pro-preview-05-06",
    canonical_slug: "google/gemini-2.5-pro-preview-03-25",
    hugging_face_id: "",
    name: "Google: Gemini 2.5 Pro Preview 05-06",
    created: 1746578513,
    description:
      "Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.",
    context_length: 1048576,
    architecture: {
      modality: "text+image-\u003Etext",
      input_modalities: ["text", "image", "file", "audio"],
      output_modalities: ["text"],
      tokenizer: "Gemini",
      instruct_type: null,
    },
    pricing: {
      prompt: "0.00000125",
      completion: "0.00001",
      request: "0",
      image: "0.00516",
      web_search: "0",
      internal_reasoning: "0",
      input_cache_read: "0.00000031",
      input_cache_write: "0.000001625",
    },
    top_provider: {
      context_length: 1048576,
      max_completion_tokens: 65535,
      is_moderated: false,
    },
    per_request_limits: null,
    supported_parameters: [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p",
    ],
  },
  {
    id: "google/gemini-2.5-flash-lite-preview-06-17",
    canonical_slug: "google/gemini-2.5-flash-lite-preview-06-17",
    hugging_face_id: "",
    name: "Google: Gemini 2.5 Flash Lite",
    created: 1750173831,
    description:
      'Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, "thinking" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. ',
    context_length: 1048576,
    architecture: {
      modality: "text+image-\u003Etext",
      input_modalities: ["file", "image", "text", "audio"],
      output_modalities: ["text"],
      tokenizer: "Gemini",
      instruct_type: null,
    },
    pricing: {
      prompt: "0.0000001",
      completion: "0.0000004",
      request: "0",
      image: "0",
      web_search: "0",
      internal_reasoning: "0",
      input_cache_read: "0.000000025",
      input_cache_write: "0.0000001833",
    },
    top_provider: {
      context_length: 1048576,
      max_completion_tokens: 65535,
      is_moderated: false,
    },
    per_request_limits: null,
    supported_parameters: [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p",
    ],
  },
  {
    id: "openrouter/sonoma-dusk-alpha",
    canonical_slug: "openrouter/sonoma-dusk-alpha",
    hugging_face_id: "",
    name: "Sonoma Dusk Alpha",
    created: 1757093247,
    description:
      "This is a cloaked model provided to the community to gather feedback. A fast and intelligent general-purpose frontier model with a 2 million token context window. Supports image inputs and parallel tool calling.\n\nNote: It’s free to use during this testing period, and prompts and completions are logged by the model creator for feedback and training.",
    context_length: 2000000,
    architecture: {
      modality: "text+image-\u003Etext",
      input_modalities: ["text", "image"],
      output_modalities: ["text"],
      tokenizer: "Other",
      instruct_type: null,
    },
    pricing: {
      prompt: "0",
      completion: "0",
      request: "0",
      image: "0",
      web_search: "0",
      internal_reasoning: "0",
    },
    top_provider: {
      context_length: 2000000,
      max_completion_tokens: null,
      is_moderated: false,
    },
    per_request_limits: null,
    supported_parameters: [
      "max_tokens",
      "response_format",
      "structured_outputs",
      "tool_choice",
      "tools",
    ],
  },
  {
    id: "openrouter/sonoma-sky-alpha",
    canonical_slug: "openrouter/sonoma-sky-alpha",
    hugging_face_id: "",
    name: "Sonoma Sky Alpha",
    created: 1757093001,
    description:
      "This is a cloaked model provided to the community to gather feedback. A maximally intelligent general-purpose frontier model with a 2 million token context window. Supports image inputs and parallel tool calling.\n\nNote: It’s free to use during this testing period, and prompts and completions are logged by the model creator for feedback and training.",
    context_length: 2000000,
    architecture: {
      modality: "text+image-\u003Etext",
      input_modalities: ["text", "image"],
      output_modalities: ["text"],
      tokenizer: "Other",
      instruct_type: null,
    },
    pricing: {
      prompt: "0",
      completion: "0",
      request: "0",
      image: "0",
      web_search: "0",
      internal_reasoning: "0",
    },
    top_provider: {
      context_length: 2000000,
      max_completion_tokens: null,
      is_moderated: false,
    },
    per_request_limits: null,
    supported_parameters: [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "structured_outputs",
      "tool_choice",
      "tools",
    ],
  },
  {
    id: "deepseek/deepseek-chat-v3.1:free",
    canonical_slug: "deepseek/deepseek-chat-v3.1",
    hugging_face_id: "deepseek-ai/DeepSeek-V3.1",
    name: "DeepSeek: DeepSeek V3.1 (free)",
    created: 1755779628,
    description:
      "DeepSeek-V3.1 is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes via prompt templates. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows. \n\nIt succeeds the [DeepSeek V3-0324](/deepseek/deepseek-chat-v3-0324) model and performs well on a variety of tasks.",
    context_length: 64000,
    architecture: {
      modality: "text-\u003Etext",
      input_modalities: ["text"],
      output_modalities: ["text"],
      tokenizer: "DeepSeek",
      instruct_type: "deepseek-v3.1",
    },
    pricing: {
      prompt: "0",
      completion: "0",
      request: "0",
      image: "0",
      web_search: "0",
      internal_reasoning: "0",
    },
    top_provider: {
      context_length: 64000,
      max_completion_tokens: null,
      is_moderated: true,
    },
    per_request_limits: null,
    supported_parameters: [
      "frequency_penalty",
      "include_reasoning",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "reasoning",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p",
    ],
  },
  {
    id: "openai/gpt-5-nano",
    canonical_slug: "openai/gpt-5-nano-2025-08-07",
    hugging_face_id: "",
    name: "OpenAI: GPT-5 Nano",
    created: 1754587402,
    description:
      "GPT-5-Nano is the smallest and fastest variant in the GPT-5 system, optimized for developer tools, rapid interactions, and ultra-low latency environments. While limited in reasoning depth compared to its larger counterparts, it retains key instruction-following and safety features. It is the successor to GPT-4.1-nano and offers a lightweight option for cost-sensitive or real-time applications.",
    context_length: 400000,
    architecture: {
      modality: "text+image-\u003Etext",
      input_modalities: ["text", "image", "file"],
      output_modalities: ["text"],
      tokenizer: "GPT",
      instruct_type: null,
    },
    pricing: {
      prompt: "0.00000005",
      completion: "0.0000004",
      request: "0",
      image: "0",
      web_search: "0.01",
      internal_reasoning: "0",
      input_cache_read: "0.000000005",
    },
    top_provider: {
      context_length: 400000,
      max_completion_tokens: 128000,
      is_moderated: true,
    },
    per_request_limits: null,
    supported_parameters: [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools",
    ],
  },
  {
    id: "deepseek/deepseek-r1-0528-qwen3-8b:free",
    canonical_slug: "deepseek/deepseek-r1-0528-qwen3-8b",
    hugging_face_id: "deepseek-ai/deepseek-r1-0528-qwen3-8b",
    name: "DeepSeek: Deepseek R1 0528 Qwen3 8B (free)",
    created: 1748538543,
    description:
      "DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek R1 that taps more compute and smarter post-training tricks, pushing its reasoning and inference to the brink of flagship models like O3 and Gemini 2.5 Pro.\nIt now tops math, programming, and logic leaderboards, showcasing a step-change in depth-of-thought.\nThe distilled variant, DeepSeek-R1-0528-Qwen3-8B, transfers this chain-of-thought into an 8 B-parameter form, beating standard Qwen3 8B by +10 pp and tying the 235 B “thinking” giant on AIME 2024.",
    context_length: 131072,
    architecture: {
      modality: "text-\u003Etext",
      input_modalities: ["text"],
      output_modalities: ["text"],
      tokenizer: "Qwen",
      instruct_type: "deepseek-r1",
    },
    pricing: {
      prompt: "0",
      completion: "0",
      request: "0",
      image: "0",
      web_search: "0",
      internal_reasoning: "0",
    },
    top_provider: {
      context_length: 131072,
      max_completion_tokens: null,
      is_moderated: false,
    },
    per_request_limits: null,
    supported_parameters: [
      "frequency_penalty",
      "include_reasoning",
      "logit_bias",
      "logprobs",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "reasoning",
      "repetition_penalty",
      "seed",
      "stop",
      "temperature",
      "top_k",
      "top_logprobs",
      "top_p",
    ],
  },
  {
    id: "google/gemini-2.5-flash-lite-preview-06-17",
    canonical_slug: "google/gemini-2.5-flash-lite-preview-06-17",
    hugging_face_id: "",
    name: "Google: Gemini 2.5 Flash Lite Preview 06-17",
    created: 1750173831,
    description:
      'Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, "thinking" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. ',
    context_length: 1048576,
    architecture: {
      modality: "text+image-\u003Etext",
      input_modalities: ["file", "image", "text", "audio"],
      output_modalities: ["text"],
      tokenizer: "Gemini",
      instruct_type: null,
    },
    pricing: {
      prompt: "0.0000001",
      completion: "0.0000004",
      request: "0",
      image: "0",
      web_search: "0",
      internal_reasoning: "0",
      input_cache_read: "0.000000025",
      input_cache_write: "0.0000001833",
    },
    top_provider: {
      context_length: 1048576,
      max_completion_tokens: 65535,
      is_moderated: false,
    },
    per_request_limits: null,
    supported_parameters: [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p",
    ],
  },
  {
    id: "openai/gpt-5",
    canonical_slug: "openai/gpt-5-2025-08-07",
    hugging_face_id: "",
    name: "OpenAI: GPT-5",
    created: 1754587413,
    description:
      'GPT-5 is OpenAI’s most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like "think hard about this." Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.',
    context_length: 400000,
    architecture: {
      modality: "text+image-\u003Etext",
      input_modalities: ["text", "image", "file"],
      output_modalities: ["text"],
      tokenizer: "GPT",
      instruct_type: null,
    },
    pricing: {
      prompt: "0.00000125",
      completion: "0.00001",
      request: "0",
      image: "0",
      web_search: "0",
      internal_reasoning: "0",
      input_cache_read: "0.000000125",
    },
    top_provider: {
      context_length: 400000,
      max_completion_tokens: 128000,
      is_moderated: true,
    },
    per_request_limits: null,
    supported_parameters: [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools",
    ],
  },
  {
    id: "moonshotai/kimi-k2:free",
    canonical_slug: "moonshotai/kimi-k2",
    hugging_face_id: "moonshotai/Kimi-K2-Instruct",
    name: "MoonshotAI: Kimi K2 (free)",
    created: 1752263252,
    description:
      "Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks. It supports long-context inference up to 128K tokens and is designed with a novel training stack that includes the MuonClip optimizer for stable large-scale MoE training.",
    context_length: 32768,
    architecture: {
      modality: "text-\u003Etext",
      input_modalities: ["text"],
      output_modalities: ["text"],
      tokenizer: "Other",
      instruct_type: null,
    },
    pricing: {
      prompt: "0",
      completion: "0",
      request: "0",
      image: "0",
      web_search: "0",
      internal_reasoning: "0",
    },
    top_provider: {
      context_length: 32768,
      max_completion_tokens: null,
      is_moderated: false,
    },
    per_request_limits: null,
    supported_parameters: [
      "frequency_penalty",
      "logit_bias",
      "logprobs",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "repetition_penalty",
      "seed",
      "stop",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_logprobs",
      "top_p",
    ],
  },
  {
    id: "openai/gpt-4.1",
    canonical_slug: "openai/gpt-4.1-2025-04-14",
    hugging_face_id: "",
    name: "OpenAI: GPT-4.1",
    created: 1744651385,
    description:
      "GPT-4.1 is a flagship large language model optimized for advanced instruction following, real-world software engineering, and long-context reasoning. It supports a 1 million token context window and outperforms GPT-4o and GPT-4.5 across coding (54.6% SWE-bench Verified), instruction compliance (87.4% IFEval), and multimodal understanding benchmarks. It is tuned for precise code diffs, agent reliability, and high recall in large document contexts, making it ideal for agents, IDE tooling, and enterprise knowledge retrieval.",
    context_length: 1047576,
    architecture: {
      modality: "text+image->text",
      input_modalities: ["image", "text", "file"],
      output_modalities: ["text"],
      tokenizer: "GPT",
      instruct_type: null,
    },
    pricing: {
      prompt: "0.000002",
      completion: "0.000008",
      request: "0",
      image: "0",
      web_search: "0",
      internal_reasoning: "0",
      input_cache_read: "0.0000005",
    },
    top_provider: { context_length: 1047576, max_completion_tokens: 32768, is_moderated: true },
  },
  {
    id: "openai/gpt-oss-20b:free",
    canonical_slug: "openai/gpt-oss-20b",
    hugging_face_id: "openai/gpt-oss-20B",
    name: "OpenAI: GTP-OSS-20b (free)",
    created: 1754414229,
    description:
      "gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware. The model is trained in OpenAI’s Harmony response format and supports reasoning level configuration, fine-tuning, and agentic capabilities including function calling, tool use, and structured outputs.",
    context_length: 131072,
    architecture: {
      modality: "text-\u003Etext",
      input_modalities: ["text"],
      output_modalities: ["text"],
      tokenizer: "GPT",
      instruct_type: null,
    },
    pricing: {
      prompt: "0",
      completion: "0",
      request: "0",
      image: "0",
      web_search: "0",
      internal_reasoning: "0",
    },
    top_provider: {
      context_length: 131072,
      max_completion_tokens: 131072,
      is_moderated: false,
    },
    per_request_limits: null,
    supported_parameters: [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "structured_outputs",
      "temperature",
      "top_p",
    ],
  },
  {
    id: "z-ai/glm-4.5-air:free",
    canonical_slug: "z-ai/glm-4.5-air",
    hugging_face_id: "zai-org/GLM-4.5-Air",
    name: "Z.AI: GLM 4.5 Air (free)",
    created: 1753471258,
    description:
      'GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for agent-centric applications. Like GLM-4.5, it adopts the Mixture-of-Experts (MoE) architecture but with a more compact parameter size. GLM-4.5-Air also supports hybrid inference modes, offering a "thinking mode" for advanced reasoning and tool use, and a "non-thinking mode" for real-time interaction. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)',
    context_length: 131072,
    architecture: {
      modality: "text-\u003Etext",
      input_modalities: ["text"],
      output_modalities: ["text"],
      tokenizer: "Other",
      instruct_type: null,
    },
    pricing: {
      prompt: "0",
      completion: "0",
      request: "0",
      image: "0",
      web_search: "0",
      internal_reasoning: "0",
    },
    top_provider: {
      context_length: 131072,
      max_completion_tokens: null,
      is_moderated: false,
    },
    per_request_limits: null,
    supported_parameters: [
      "frequency_penalty",
      "include_reasoning",
      "logit_bias",
      "logprobs",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "reasoning",
      "repetition_penalty",
      "seed",
      "stop",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_logprobs",
      "top_p",
    ],
  },
  {
    id: "perplexity/sonar",
    canonical_slug: "perplexity/sonar",
    hugging_face_id: "",
    name: "Perplexity: Sonar",
    created: 1738013808,
    description:
      "Sonar is lightweight, affordable, fast, and simple to use — now featuring citations and the ability to customize sources. It is designed for companies seeking to integrate lightweight question-and-answer features optimized for speed.",
    context_length: 127072,
    architecture: {
      modality: "text+image->text",
      input_modalities: ["text", "image"],
      output_modalities: ["text"],
      tokenizer: "Other",
      instruct_type: null,
    },
    pricing: {
      prompt: "0.000001",
      completion: "0.000001",
      request: "0.005",
      image: "0",
      web_search: "0",
      internal_reasoning: "0",
    },
  },
]

export default models
